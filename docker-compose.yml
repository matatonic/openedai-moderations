services:
  openedai-moderations:
    build:
      dockerfile: Dockerfile
    image: ghcr.io/matatonic/openedai-moderations
    env_file:
      - moderations.env
    environment:
      - OPENAI_BASE_URL=http://text-embeddings-inference:80/v1
    stdin_open: true
    tty: true
    ports:
      - 5002:5002
    command: python moderations.py

  # Comment this to use another embeddings API
  text-embeddings-inference:
    extends:
      file: docker-compose.tei.yml
      service: text-embeddings-inference
